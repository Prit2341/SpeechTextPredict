{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Load and tokenize the data\n",
    "data = open('clean_data.txt', 'r').read()\n",
    "\n",
    "# Tokenize the data and fit it to the text\n",
    "tokenizer = Tokenizer(oov_token='<oov>')\n",
    "tokenizer.fit_on_texts(data.split())\n",
    "\n",
    "# Save tokenizer\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Mobile voice to text and predictive text\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.0337 - loss: 6.9172\n",
      "Epoch 1: loss improved from inf to 6.36203, saving model to model.keras\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 1s/step - accuracy: 0.0340 - loss: 6.9075 - learning_rate: 0.0100\n",
      "Epoch 2/20\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.1074 - loss: 5.4981\n",
      "Epoch 2: loss improved from 6.36203 to 5.40065, saving model to model.keras\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 1s/step - accuracy: 0.1075 - loss: 5.4964 - learning_rate: 0.0100\n",
      "Epoch 3/20\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.1439 - loss: 4.9015\n",
      "Epoch 3: loss improved from 5.40065 to 4.89671, saving model to model.keras\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 1s/step - accuracy: 0.1440 - loss: 4.9014 - learning_rate: 0.0100\n",
      "Epoch 4/20\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.1769 - loss: 4.3444\n",
      "Epoch 4: loss improved from 4.89671 to 4.39840, saving model to model.keras\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 1s/step - accuracy: 0.1768 - loss: 4.3453 - learning_rate: 0.0100\n",
      "Epoch 5/20\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.2421 - loss: 3.6296\n",
      "Epoch 5: loss improved from 4.39840 to 3.73759, saving model to model.keras\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 1s/step - accuracy: 0.2419 - loss: 3.6315 - learning_rate: 0.0100\n",
      "Epoch 6/20\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3626 - loss: 2.8103\n",
      "Epoch 6: loss improved from 3.73759 to 2.92025, saving model to model.keras\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 1s/step - accuracy: 0.3623 - loss: 2.8122 - learning_rate: 0.0100\n",
      "Epoch 7/20\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.5168 - loss: 2.0111\n",
      "Epoch 7: loss improved from 2.92025 to 2.12344, saving model to model.keras\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 1s/step - accuracy: 0.5163 - loss: 2.0130 - learning_rate: 0.0100\n",
      "Epoch 8/20\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.6664 - loss: 1.3475\n",
      "Epoch 8: loss improved from 2.12344 to 1.44539, saving model to model.keras\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 1s/step - accuracy: 0.6659 - loss: 1.3492 - learning_rate: 0.0100\n",
      "Epoch 9/20\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7908 - loss: 0.8484\n",
      "Epoch 9: loss improved from 1.44539 to 0.92050, saving model to model.keras\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 1s/step - accuracy: 0.7904 - loss: 0.8497 - learning_rate: 0.0100\n",
      "Epoch 10/20\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8852 - loss: 0.4968\n",
      "Epoch 10: loss improved from 0.92050 to 0.54887, saving model to model.keras\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 1s/step - accuracy: 0.8848 - loss: 0.4978 - learning_rate: 0.0100\n",
      "Epoch 11/20\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9416 - loss: 0.2875\n",
      "Epoch 11: loss improved from 0.54887 to 0.31523, saving model to model.keras\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 1s/step - accuracy: 0.9415 - loss: 0.2879 - learning_rate: 0.0100\n",
      "Epoch 12/20\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9756 - loss: 0.1573\n",
      "Epoch 12: loss improved from 0.31523 to 0.17412, saving model to model.keras\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 1s/step - accuracy: 0.9756 - loss: 0.1576 - learning_rate: 0.0100\n",
      "Epoch 13/20\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9899 - loss: 0.0890\n",
      "Epoch 13: loss improved from 0.17412 to 0.09903, saving model to model.keras\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 1s/step - accuracy: 0.9898 - loss: 0.0892 - learning_rate: 0.0100\n",
      "Epoch 14/20\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9943 - loss: 0.0540\n",
      "Epoch 14: loss improved from 0.09903 to 0.06168, saving model to model.keras\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 1s/step - accuracy: 0.9942 - loss: 0.0542 - learning_rate: 0.0100\n",
      "Epoch 15/20\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9948 - loss: 0.0431\n",
      "Epoch 15: loss improved from 0.06168 to 0.04900, saving model to model.keras\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 1s/step - accuracy: 0.9947 - loss: 0.0432 - learning_rate: 0.0100\n",
      "Epoch 16/20\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9948 - loss: 0.0336\n",
      "Epoch 16: loss improved from 0.04900 to 0.03899, saving model to model.keras\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 1s/step - accuracy: 0.9948 - loss: 0.0337 - learning_rate: 0.0100\n",
      "Epoch 17/20\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9950 - loss: 0.0302\n",
      "Epoch 17: loss improved from 0.03899 to 0.03601, saving model to model.keras\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 1s/step - accuracy: 0.9950 - loss: 0.0303 - learning_rate: 0.0100\n",
      "Epoch 18/20\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9953 - loss: 0.0258\n",
      "Epoch 18: loss improved from 0.03601 to 0.03241, saving model to model.keras\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 1s/step - accuracy: 0.9953 - loss: 0.0259 - learning_rate: 0.0100\n",
      "Epoch 19/20\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9955 - loss: 0.0245\n",
      "Epoch 19: loss improved from 0.03241 to 0.03062, saving model to model.keras\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 1s/step - accuracy: 0.9955 - loss: 0.0246 - learning_rate: 0.0100\n",
      "Epoch 20/20\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9951 - loss: 0.0237\n",
      "Epoch 20: loss improved from 0.03062 to 0.02996, saving model to model.keras\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 1s/step - accuracy: 0.9951 - loss: 0.0238 - learning_rate: 0.0100\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.utils import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Embedding, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create input sequences from the data\n",
    "def create_input_sequences(data, n_gram_size=6):\n",
    "    # Create n-gram input sequences based on an n-gram size of 6\n",
    "    input_sequences = []\n",
    "    token_list = tokenizer.texts_to_sequences([data])[0]\n",
    "\n",
    "    # Sliding iteration which takes every 6 words in a row as an input sequence\n",
    "    for i in range(1, len(token_list) - n_gram_size):\n",
    "        n_gram_sequence = token_list[i:i+n_gram_size]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "    # Pad sequences\n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    return np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')), max_sequence_len\n",
    "\n",
    "# Create the features and labels, and split the data into training and testing sets\n",
    "def create_training_data(input_sequences):\n",
    "    # Create features and labels\n",
    "    xs, labels = input_sequences[:,:-1], input_sequences[:,-1]\n",
    "    ys = to_categorical(labels, num_classes=total_words)\n",
    "\n",
    "    # Split data\n",
    "    return train_test_split(xs, ys, test_size=0.1, shuffle=True)\n",
    "\n",
    "# Train the model\n",
    "def train_model(X_train, y_train, total_words, max_sequence_len):\n",
    "    # Create callbacks\n",
    "    checkpoint = ModelCheckpoint(\"model.keras\", monitor='loss', verbose=1, save_best_only=True, mode='auto')\n",
    "    reduce = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001, verbose=1)\n",
    "\n",
    "    # Create optimizer\n",
    "    optimizer = Adam(learning_rate=0.01)\n",
    "\n",
    "    # Create model\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
    "    model.add(Bidirectional(LSTM(512)))\n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    model.fit(\n",
    "        X_train, y_train, epochs=20, batch_size=2000,\n",
    "        callbacks=[checkpoint, reduce]\n",
    "    )\n",
    "\n",
    "# Load cleaned data\n",
    "data = open('clean_data.txt', 'r').read().split(' ')\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Create input sequences and training data\n",
    "input_sequences, max_sequence_len = create_input_sequences(data)\n",
    "X_train, X_test, y_train, y_test = create_training_data(input_sequences)\n",
    "\n",
    "# Train the model\n",
    "train_model(X_train, y_train, total_words, max_sequence_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from keras.models import load_model\n",
    "\n",
    "model = None\n",
    "tokenizer = None\n",
    "\n",
    "def load_tokenizer():\n",
    "    global tokenizer\n",
    "    if tokenizer is None:\n",
    "        # Load the tokenizer\n",
    "        with open('tokenizer.pickle', 'rb') as handle:\n",
    "            tokenizer = pickle.load(handle)\n",
    "    return tokenizer\n",
    "\n",
    "def load_model():\n",
    "    global model\n",
    "    if model is None:\n",
    "        # Load the model\n",
    "        model = load_model('model.keras')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Predict text based on a set of seed text\n",
    "def predict_text(seed_text):\n",
    "    # Convert the seed text into a token list using the same process as the previous tokenization\n",
    "    load_tokenizer()\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=5, padding='pre')\n",
    "\n",
    "    # Make the prediction\n",
    "    m = load_model()\n",
    "    predict_x = m.predict(token_list, batch_size=500, verbose=0)\n",
    "\n",
    "    # Find the top three words\n",
    "    predict_x = np.argpartition(predict_x, -3, axis=1)[0][-3:]\n",
    "\n",
    "    # Reverse the list so the most popular is first\n",
    "    predictions = list(predict_x)\n",
    "    predictions.reverse()\n",
    "\n",
    "    # Iterate over the predicted words, and find the word in the tokenizer dictionary that matches\n",
    "    output_words = []\n",
    "    for prediction in predictions:\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if prediction == index:\n",
    "                output_words.append(word)\n",
    "                break\n",
    "\n",
    "    return output_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001B91EDEE160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Predictions for 'where': ['he', 'prevented', 'their']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Load the tokenizer\n",
    "def load_tokenizer():\n",
    "    with open('tokenizer.pickle', 'rb') as handle:\n",
    "        tokenizer = pickle.load(handle)\n",
    "    return tokenizer\n",
    "\n",
    "# Load the model\n",
    "def load_trained_model():\n",
    "    model = load_model('model.keras')\n",
    "    return model\n",
    "\n",
    "# Function to predict text\n",
    "def predict_text(seed_text, tokenizer, model):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=5, padding='pre')\n",
    "    predict_x = model.predict(token_list, batch_size=500, verbose=0)\n",
    "    predict_x = np.argpartition(predict_x, -3, axis=1)[0][-3:]\n",
    "    predictions = list(predict_x)\n",
    "    predictions.reverse()\n",
    "    output_words = []\n",
    "    for prediction in predictions:\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if prediction == index:\n",
    "                output_words.append(word)\n",
    "                break\n",
    "    return output_words\n",
    "\n",
    "# Main function to run the test\n",
    "def main():\n",
    "    seed_text = \"where\"\n",
    "    tokenizer = load_tokenizer()\n",
    "    model = load_trained_model()\n",
    "    predictions = predict_text(seed_text, tokenizer, model)\n",
    "    print(f\"Predictions for '{seed_text}': {predictions}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
